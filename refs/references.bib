@misc{ha2018worldmodels,
  title         = {World Models},
  author        = {Ha, David and Schmidhuber, J{\"u}rgen},
  year          = {2018},
  doi           = {10.5281/zenodo.1207631},
  url           = {https://zenodo.org/record/1207631},
  note          = {arXiv:1803.10122},
  annote        = {Seminal modern world-model paper: learn latent dynamics (VAE + RNN) and train policies using model features or entirely “in imagination,” influencing a large wave of model-based RL.}
}

@misc{worldmodels2018interactive,
  title  = {World Models (Interactive Project Page)},
  author = {Ha, David and Schmidhuber, J{\"u}rgen},
  year   = {2018},
  url    = {https://worldmodels.github.io/},
  annote = {Canonical companion site with explanations and demos; great link target for an Awesome list entry.}
}

@software{ha2018worldmodelsexperiments,
  title  = {World Models Experiments},
  author = {Ha, David},
  year   = {2018},
  url    = {https://github.com/hardmaru/WorldModelsExperiments},
  annote = {Reference implementation used widely to reproduce the World Models experiments and learn the pipeline end-to-end.}
}

@online{ha2018worldmodelsexperimentsblog,
  title  = {World Models Experiments (Blog Post)},
  author = {Ha, David},
  year   = {2018},
  month  = jun,
  url    = {https://blog.otoro.net/2018/06/09/world-models-experiments/},
  annote = {Step-by-step reproduction guide; extremely useful for practitioners and readers of the original paper.}
}

@inproceedings{ha2018recurrentworldmodels,
  title     = {Recurrent World Models Facilitate Policy Evolution},
  author    = {Ha, David and Schmidhuber, J{\"u}rgen},
  booktitle = {Advances in Neural Information Processing Systems 31 (NeurIPS 2018)},
  year      = {2018},
  url       = {https://arxiv.org/abs/1809.01999},
  note      = {Proceedings record: 10.5555/3327144.3327171},
  annote    = {Shows training/optimizing policies inside a learned recurrent world model and transferring back to the real env; key for “imagination-based” control.}
}

@inproceedings{schmidhuber1990onlinealgorithm,
  title     = {An On-Line Algorithm for Dynamic Reinforcement Learning and Planning in Reactive Environments},
  author    = {Schmidhuber, J{\"u}rgen},
  booktitle = {Proceedings of the International Joint Conference on Neural Networks (IJCNN)},
  year      = {1990},
  pages     = {253--258},
  doi       = {10.1109/IJCNN.1990.137723},
  annote    = {Early differentiable model-based RL: recurrent world model + planning/control ideas that anticipate later “world model” lines of work.}
}

@techreport{schmidhuber1990makingworlddifferentiable,
  title       = {Making the World Differentiable: On Using Self-Supervised Fully Recurrent Neural Networks for Dynamic Reinforcement Learning and Planning in Non-Stationary Environments},
  author      = {Schmidhuber, J{\"u}rgen},
  institution = {Institut f{\"u}r Informatik, Technische Universit{\"a}t M{\"u}nchen},
  year        = {1990},
  number      = {FKI-126-90},
  annote      = {Classic technical report on training recurrent predictive models of the environment and using them for RL/planning under non-stationarity.}
}

@inproceedings{schmidhuber1991curiousmodelbuilding,
  title     = {Curious Model-Building Control Systems},
  author    = {Schmidhuber, J{\"u}rgen},
  booktitle = {Proceedings of the International Joint Conference on Neural Networks (IJCNN)},
  year      = {1991},
  pages     = {1458--1463},
  annote    = {A core intrinsic-motivation formulation: agents act to improve their world model (curiosity), shaping modern exploration methods in RL.}
}

@inproceedings{schmidhuber1991curiosityboredom,
  title     = {A Possibility for Implementing Curiosity and Boredom in Model-Building Neural Controllers},
  author    = {Schmidhuber, J{\"u}rgen},
  booktitle = {From Animals to Animats: Proceedings of the First International Conference on Simulation of Adaptive Behavior},
  year      = {1991},
  pages     = {222--227},
  publisher = {MIT Press},
  doi       = {10.7551/mitpress/3115.003.0030},
  annote    = {Foundational intrinsic reward idea (novelty / learning progress) for exploration—still central in curiosity-driven RL.}
}

@inproceedings{DBLP:conf/nips/Schmidhuber90,
  author       = {J{\"{u}}rgen Schmidhuber},
  editor       = {Richard Lippmann and John E. Moody and David S. Touretzky},
  title        = {Reinforcement Learning in Markovian and Non-Markovian Environments},
  booktitle    = {Advances in Neural Information Processing Systems 3 (NIPS 1990)},
  pages        = {500--506},
  publisher    = {Morgan Kaufmann},
  year         = {1990},
  url          = {http://papers.nips.cc/paper/393-reinforcement-learning-in-markovian-and-non-markovian-environments},
  annote       = {Early treatment of RL under partial observability/non-Markov settings—an enduring theme for world models + memory-based agents.}
}

@article{schmidhuber1992historycompression,
  title   = {Learning Complex, Extended Sequences Using the Principle of History Compression},
  author  = {Schmidhuber, J{\"u}rgen},
  journal = {Neural Computation},
  year    = {1992},
  volume  = {4},
  number  = {2},
  pages   = {234--242},
  doi     = {10.1162/neco.1992.4.2.234},
  annote  = {Memory and representation learning for long temporal dependencies; highly relevant to POMDP RL and predictive world modeling.}
}

@article{wiering1997hqlearning,
  title   = {HQ-Learning},
  author  = {Wiering, Marco and Schmidhuber, J{\"u}rgen},
  journal = {Adaptive Behavior},
  year    = {1997},
  volume  = {6},
  number  = {2},
  pages   = {219--246},
  doi     = {10.1177/105971239700600202},
  annote  = {Hierarchical / decomposition-flavored RL ideas for POMDP-like settings; helps connect memory + hierarchy themes in Schmidhuber’s RL work.}
}

@article{hochreiter1997lstm,
  title   = {Long Short-Term Memory},
  author  = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal = {Neural Computation},
  year    = {1997},
  volume  = {9},
  number  = {8},
  pages   = {1735--1780},
  doi     = {10.1162/neco.1997.9.8.1735},
  annote  = {Core sequence-memory architecture; enables practical RL in partially observable environments and underpins many world-model approaches.}
}

@article{sehnke2010pgpe,
  title   = {Parameter-Exploring Policy Gradients},
  author  = {Sehnke, Frank and Osendorfer, Christian and R{\"u}ckstiess, Thomas and Graves, Alex and Peters, Jan and Schmidhuber, J{\"u}rgen},
  journal = {Neural Networks},
  year    = {2010},
  volume  = {23},
  number  = {4},
  pages   = {551--559},
  doi     = {10.1016/j.neunet.2009.12.004},
  annote  = {Influential policy-search method (parameter-space exploration) for RL/POMDP control with lower-variance gradient estimates.}
}

@article{schmidhuber2010intrinsicmotivation,
  title   = {Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990--2010)},
  author  = {Schmidhuber, J{\"u}rgen},
  journal = {IEEE Transactions on Autonomous Mental Development},
  year    = {2010},
  volume  = {2},
  number  = {3},
  pages   = {230--247},
  doi     = {10.1109/TAMD.2010.2056368},
  annote  = {Theory backbone for curiosity-driven exploration: intrinsic reward via compression progress / learning progress; widely cited in intrinsic-motivation RL.}
}

@article{schmidhuber2006developmentalrobotics,
  title   = {Developmental Robotics, Optimal Artificial Curiosity, Creativity, Music, and the Fine Arts},
  author  = {Schmidhuber, J{\"u}rgen},
  journal = {Connection Science},
  year    = {2006},
  volume  = {18},
  number  = {2},
  pages   = {173--187},
  doi     = {10.1080/09540090600768658},
  annote  = {Accessible synthesis of optimal curiosity and intrinsic motivation ideas that later reappear across exploration-heavy RL methods.}
}

@article{schmidhuber2015deeplearningoverview,
  title   = {Deep Learning in Neural Networks: An Overview},
  author  = {Schmidhuber, J{\"u}rgen},
  journal = {Neural Networks},
  year    = {2015},
  volume  = {61},
  pages   = {85--117},
  doi     = {10.1016/j.neunet.2014.09.003},
  annote  = {Broad, highly cited overview that also connects deep learning, sequence models, and RL threads (useful “bridge reference” in an Awesome RL repo).}
}

@article{wierstra2014nes,
  title   = {Natural Evolution Strategies},
  author  = {Wierstra, Daan and Schaul, Tom and Peters, Jan and Schmidhuber, J{\"u}rgen},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  pages   = {949--980},
  annote  = {Key neuroevolution / black-box optimization method often used as an RL alternative; important in the ES-vs-RL lineage and policy search.}
}

@inproceedings{cuccu2011intrinsicneuroevolution,
  title     = {Intrinsically Motivated Neuroevolution for Vision-Based Reinforcement Learning},
  author    = {Cuccu, Giuseppe and Luciw, Matthew and Schmidhuber, J{\"u}rgen and Gomez, Faustino},
  booktitle = {2011 IEEE International Conference on Development and Learning (ICDL)},
  year      = {2011},
  doi       = {10.1109/DEVLRN.2011.6037324},
  annote    = {Combines intrinsic motivation + neuroevolution for RL from pixels; relevant precursor to modern curiosity+representation learning pipelines.}
}

@inproceedings{koutnik2013evolving,
  title     = {Evolving Large-Scale Neural Networks for Vision-Based Reinforcement Learning},
  author    = {Koutn{\'i}k, Jan and Cuccu, Giuseppe and Schmidhuber, J{\"u}rgen and Gomez, Faustino},
  booktitle = {Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation (GECCO '13)},
  year      = {2013},
  pages     = {1061--1068},
  doi       = {10.1145/2463372.2463509},
  annote    = {High-impact neuroevolution + RL from raw pixels; helps ground the “evolutionary RL / policy search” branch tied to IDSIA work.}
}

@article{schmidhuber2015learning,
  title         = {On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models},
  author        = {Schmidhuber, Juergen},
  year          = {2015},
  month         = nov,
  eprint        = {1511.09249},
  archivePrefix = {arXiv},
  primaryClass  = {cs.AI},
  doi           = {10.48550/arXiv.1511.09249},
  url           = {https://arxiv.org/abs/1511.09249},
  note          = {Submitted on 30 Nov 2015}
}

@article{behrouz2024titans,
  title         = {Titans: Learning to Memorize at Test Time},
  author        = {Behrouz, Ali and Zhong, Peilin and Mirrokni, Vahab},
  year          = {2024},
  eprint        = {2501.00663},
  archivePrefix = {arXiv},
  primaryClass  = {cs.LG},
  doi           = {10.48550/arXiv.2501.00663},
  url           = {https://arxiv.org/abs/2501.00663}
}


@article{mnih2015dqn,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  doi={10.1038/nature14236}
}

@article{schulman2017ppo,
  title={Proximal Policy Optimization Algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017},
  url={https://arxiv.org/abs/1707.06347}
}

@book{sutton2018reinforcement,
  title     = {Reinforcement Learning: An Introduction},
  author    = {Sutton, Richard S. and Barto, Andrew G.},
  publisher = {MIT Press},
  year      = {2018},
  annote    = {The foundational textbook in RL, covering theory and core algorithms such as dynamic programming, TD learning, and policy gradient methods — essential for anyone studying RL.}
}

@article{sutton1988tdlearning,
  title   = {Learning to predict by the methods of temporal differences},
  author  = {Sutton, Richard S.},
  journal = {Machine Learning},
  year    = {1988},
  volume  = {3},
  number  = {1},
  pages   = {9--44},
  annote  = {Introduces temporal-difference (TD) learning, a cornerstone of RL that precedes and underlies many later methods such as Q-learning.}
}

@techreport{watkins1992qlearning,
  title       = {Q-Learning},
  author      = {Watkins, Christopher J. C. H. and Dayan, Peter},
  institution = {Cambridge University Engineering Department},
  year        = {1992},
  annote      = {Introduces Q-learning, a foundational off-policy RL algorithm for discrete action spaces; basis for many later extensions (e.g., DQN).}
}

@article{williams1992reinforce,
  title   = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author  = {Williams, Ronald J.},
  journal = {Machine Learning},
  year    = {1992},
  volume  = {8},
  number  = {3--4},
  pages   = {229--256},
  annote  = {Defines the REINFORCE policy gradient algorithm, a foundational on-policy method directly optimizing expected returns.}
}

@article{tesauro1995tdgammon,
  title   = {Temporal difference learning and TD-Gammon},
  author  = {Tesauro, Gerald},
  journal = {Communications of the ACM},
  year    = {1995},
  volume  = {38},
  number  = {3},
  pages   = {58--68},
  annote  = {A pioneering neural network + RL system that achieved expert-level backgammon play using TD(λ), showing the power of combining learning with neural networks.}
}

@inproceedings{mnih2015dqn,
  title     = {Human level control through deep reinforcement learning},
  author    = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and others},
  booktitle = {Nature},
  year      = {2015},
  annote    = {The Deep Q-Network (DQN) paper that ignited modern deep reinforcement learning by training an end-to-end neural agent on Atari games from pixels.}
}

@article{vanhasselt2016double,
  title   = {Deep Reinforcement Learning with Double Q-Learning},
  author  = {Van Hasselt, Hado and Guez, Arthur and Silver, David},
  journal = {AAAI},
  year    = {2016},
  annote  = {Introduces Double DQN to reduce overestimation bias in Q-learning, improving stability and performance in deep RL.}
}

@inproceedings{hessel2018rainbow,
  title     = {Rainbow: Combining Improvements in Deep Reinforcement Learning},
  author    = {Hessel, Matteo and Modayil, Joseph and Van Hasselt, Hado and others},
  booktitle = {AAAI},
  year      = {2018},
  annote    = {Unified “Rainbow” agent that integrates multiple DQN enhancements (Double, Prioritized Replay, Distributional, etc.), setting a strong benchmark in deep RL.}
}

@inproceedings{mnih2016asynchronous,
  title     = {Asynchronous Methods for Deep Reinforcement Learning},
  author    = {Mnih, Volodymyr and Badia, Adri{\`a} Puigdom{\`e}nech and Mirza, Mehdi and others},
  booktitle = {ICML},
  year      = {2016},
  annote    = {Introduces A3C, a scalable parallel policy gradient method that helped the field move beyond purely value-based deep RL.}
}

@article{schulman2015trust,
  title   = {Trust Region Policy Optimization},
  author  = {Schulman, John and Levine, Sergey and Moritz, Philipp and others},
  journal = {ICML},
  year    = {2015},
  annote  = {TRPO improves policy gradient learning by enforcing trust regions for more stable updates, a precursor to PPO.}
}

@article{schulman2017ppo,
  title   = {Proximal Policy Optimization Algorithms},
  author  = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and others},
  journal = {arXiv preprint},
  year    = {2017},
  annote  = {PPO presents a simpler and efficient policy gradient method widely used in modern deep RL applications due to its robustness and performance.}
}

@article{lillicrap2015ddpg,
  title   = {Continuous control with deep reinforcement learning},
  author  = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and others},
  journal = {arXiv preprint},
  year    = {2015},
  annote  = {Deep Deterministic Policy Gradient (DDPG) extends deep RL to high-dimensional continuous action spaces using an actor-critic approach.}
}

@inproceedings{silver2016alphago,
  title     = {Mastering the game of Go with deep neural networks and tree search},
  author    = {Silver, David and Huang, Aja and Maddison, Chris J. and others},
  booktitle = {Nature},
  year      = {2016},
  annote    = {AlphaGo demonstration of combining deep RL with Monte Carlo Tree Search to reach superhuman performance, cementing RL’s prominence in game AI.}
}

@inproceedings{silver2017alphagozero,
  title     = {Mastering the game of Go without human knowledge},
  author    = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and others},
  booktitle = {Nature},
  year      = {2017},
  annote    = {AlphaGo Zero learns tabula-rasa with self-play and reinforcement learning, illustrating the power of purely algorithmic learning.}
}

@article{haarnoja2017softq,
  title   = {Reinforcement Learning with Deep Energy-Based Policies},
  author  = {Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  journal = {arXiv preprint},
  year    = {2017},
  annote  = {Soft Q-learning / maximum entropy RL, advancing exploration and robustness in continuous control.}
}

@article{tapenade2025survey,
  title   = {Comprehensive Survey of Reinforcement Learning},
  author  = {Terven, J.},
  journal = {MDPI},
  year    = {2025},
  annote  = {A recent survey covering traditional RL to modern deep RL, valuable for mapping trends and research directions.}
}

@software{stooke2019rlpyt,
  title  = {rlpyt: A Research Code Base for Deep Reinforcement Learning in PyTorch},
  author = {Stooke, Adam and Abbeel, Pieter},
  year   = {2019},
  url    = {https://github.com/astooke/rlpyt},
  annote = {A comprehensive research codebase implementing many common deep RL algorithms for experimentation and benchmarking.}
}

@online{allenpandas_rlpapers,
  title  = {Allenpandas/Reinforcement-Learning-Papers},
  author = {{Allenpandas}},
  year   = {2025},
  url    = {https://github.com/Allenpandas/Reinforcement-Learning-Papers},
  annote = {Curated GitHub repo of notable RL research papers across top conferences — great companion list for an Awesome RL bibliography.}
}

@online{yingchengyang_rlpapers,
  title  = {Reinforcement Learning Papers (yingchengyang)},
  author = {{yingchengyang}},
  year   = {2025},
  url    = {https://github.com/yingchengyang/Reinforcement-Learning-Papers},
  annote = {Another community-maintained GitHub list covering classic and recent RL research across model-free and model-based areas.}
}
